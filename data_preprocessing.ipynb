{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.feather as feather\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATION_DIR = \"/soe/narehman/docker_home/simulations/ml_data/datacenter/datacenter/\"\n",
    "WORKLOAD_SIMPOINTS = {'clang' : [812],\n",
    "             'gcc' : [939],\n",
    "             'mongodb' : [4118],\n",
    "             'mysql' : [1172],\n",
    "             'postgres' : [2807],\n",
    "             'verilator' : [31568],\n",
    "             'xgboost' : [3311],}\n",
    "EXP_NAME = \"data\"\n",
    "CONFIG_NAME = \"data\"\n",
    "SUITE_NAME = \"datacenter\"\n",
    "SUBSUITE_NAME = \"datacenter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_COMPRESSSED_DATA_DIR = \"raw_compressed_data/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Data stored un-normalized in workload.feathers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPRESSION\n",
    "- convert hex to int\n",
    "- store to feather files for quick loading later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert hex addresses to ints\n",
    "def hex_to_int(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return int(value, 16) \n",
    "        except ValueError:\n",
    "            return float('nan')  \n",
    "    return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csvs():\n",
    "    csvs = {}\n",
    "    for workload in WORKLOAD_SIMPOINTS.keys():\n",
    "        csvs[workload] = []\n",
    "        for simpoint in WORKLOAD_SIMPOINTS[workload]:\n",
    "            csvs[workload].append(f\"{SIMULATION_DIR}/{workload}/{simpoint}/ml_data.csv\")\n",
    "    return csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should I compress it by simpoint?\n",
    "def compress_raw_data():\n",
    "    workload_csvs = get_csvs()\n",
    "    for workload, csvs in workload_csvs.items():\n",
    "        data = pd.concat([pd.read_csv(csv) for csv in csvs], ignore_index=True)\n",
    "        #FIXME: handle this in scarab\n",
    "        data['ft_start_addr'] = data['ft_start_addr'].apply(lambda x: hex_to_int(x))\n",
    "        data.to_feather(f'{RAW_COMPRESSSED_DATA_DIR}/{workload}.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE METADATA COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATED_DATA_DIR = \"icache_consumed_data/\" # stores data with generated metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_data(df):\n",
    "    # Calc first ft after resteer\n",
    "    df['cycles_since_rec_agg'] = df['cycles_since_btb_rec'] * df['cycles_since_ibtb_rec'] * df['cycles_since_misfetch_rec'] * df['cycles_since_mispred_rec']\n",
    "    mask = df['off_path_reason'] > 0\n",
    "    df['first_after_resteer'] = mask\n",
    "    df['first_after_resteer'] = df['first_after_resteer'].astype(int)\n",
    "\n",
    "    # calc window_id (window is defined as the time between two resteers)\n",
    "    df['window_id'] = df['first_after_resteer'].cumsum()\n",
    "    # get window lenth (in fts)\n",
    "    df['length_window'] = df.groupby('window_id')['off_path'].transform('count')\n",
    "    # get len off-path\n",
    "    df['length_off_path'] = df[df['off_path'] == 1].groupby('window_id')['off_path'].transform('count')\n",
    "    df['length_off_path'] = df['length_off_path'].bfill().ffill().astype(int)\n",
    "    # Compute the distance from the first 1 in each group\n",
    "    df['penalty'] = 1\n",
    "    # ft penalty\n",
    "    df.loc[df['off_path'] == 0, 'penalty'] = df[df['off_path'] == 0].groupby(['window_id', 'off_path']).cumcount(ascending=False) + 1\n",
    "    df.loc[df['off_path'] == 1, 'penalty'] = df[df['off_path'] == 1].groupby(['window_id', 'off_path']).cumcount()\n",
    "    # cycle penalty\n",
    "    # TODO: ???\n",
    "    df['icache_cycle'] = df['icache_cycle'].replace(18446744073709551615, np.nan)\n",
    "    mispred_cycle_per_window = df.loc[df['consumed_icache'] == 1].loc[df['off_path_reason'] > 0].groupby('window_id')['icache_cycle'].first()\n",
    "    df['icache_cycle_mispred'] = df['window_id'].map(mispred_cycle_per_window)\n",
    "    df['cycle_penalty'] = (df['icache_cycle_mispred'] - df['icache_cycle']).abs()\n",
    "\n",
    "    last_off_path_cycle = df.loc[df['consumed_icache'] == 1].groupby('window_id').last()['icache_cycle']\n",
    "    df['icache_cycle_last_offpath'] = df['window_id'].map(last_off_path_cycle)\n",
    "    df['off_path_cycles'] = df['icache_cycle_last_offpath'] - df['icache_cycle_mispred']\n",
    "\n",
    "    first_on_path_cycle = df.loc[df['consumed_icache'] == 1].groupby('window_id').first()['icache_cycle']\n",
    "    df['icache_cycle_first_onpath'] = df['window_id'].map(first_on_path_cycle)\n",
    "    df['on_path_cycles'] = df['icache_cycle_mispred'] - df['icache_cycle_first_onpath']\n",
    "    '''\n",
    "    # off_path len in cycles \n",
    "    df = pd.merge(df, df[df['window_id'] != df['window_id'].shift][['window_id', 'icache_cycle']], on='window_id', how='left', suffixes=('', '_last_offpath'))\n",
    "    df['off_path_cycles'] = df['icache_cycle_last_offpath'] - df['icache_cycle_mispred']\n",
    "    '''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(df, workload):\n",
    "    count = len(df)\n",
    "    count_pos = len(df[df['off_path'] == 1])\n",
    "    count_neg = len(df[df['off_path'] == 0])\n",
    "    return {workload : { 'length' : count, 'pos' : count_pos, 'neg' : count_neg}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clang\n",
      "gcc\n",
      "mysql\n",
      "mongodb\n",
      "postgres\n",
      "verilator\n",
      "xgboost\n"
     ]
    }
   ],
   "source": [
    "# TODO: fix how these are generated\n",
    "workload_feathers = {\n",
    "    'clang' : ['raw_data_compressed/clang.feather'],\n",
    "    'gcc' : ['raw_data_compressed/gcc.feather',],\n",
    "    'mysql' : ['raw_data_compressed/mysql.feather',],\n",
    "    'mongodb' : ['raw_data_compressed/mongodb.feather',],\n",
    "    'postgres' : ['raw_data_compressed/postgres.feather',],\n",
    "    'verilator' : ['raw_data_compressed/verilator.feather',],\n",
    "    'xgboost' : ['raw_data_compressed/xgboost.feather',]\n",
    "}\n",
    "\n",
    "metadata = {}\n",
    "for workload, feathers in workload_feathers.items():\n",
    "    print(workload)\n",
    "    df = pd.concat([pd.read_feather(feather) for feather in feathers])\n",
    "    df = annotate_data(df)\n",
    "    metadata |= get_metadata(df, workload)\n",
    "    df.to_feather(f'{ANNOTATED_DATA_DIR}/{workload}.feather')\n",
    "\n",
    "with open(f'{ANNOTATED_DATA_DIR}/metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHUNK DATA\n",
    "Split data into chunks (train/test split is allocated at the chunk granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 10000\n",
    "UNNORMALIZED_CHUNKED_DATA_DIR = \"icache_consumed_chunked_data_raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_metadata_dict(df, workload, id):\n",
    "    length = len(df)\n",
    "    num_pos = len(df[df['off_path'] == 1])\n",
    "    num_neg = len(df[df['off_path'] == 0])\n",
    "    return {id: {'workload' : workload, 'length' : length, 'num_pos' : num_pos, 'num_neg' : num_neg}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(workload_feathers):\n",
    "    chunk_id = 0\n",
    "    metadata_dict = {}\n",
    "    for workload, feathers in workload_feathers.items():\n",
    "        print(f'chunking {workload}')\n",
    "        metadata = {}\n",
    "        for simp in feathers:\n",
    "            df = pd.read_feather(simp)\n",
    "            df['workload'] = workload\n",
    "            df['chunk_id'] = df.index // CHUNK_SIZE \n",
    "            chunks = [group for _, group in df.groupby('chunk_id')]\n",
    "            del(df)\n",
    "            for chunk in chunks:\n",
    "                chunk.drop('chunk_id', axis=1, inplace=True)\n",
    "                metadata|= make_metadata_dict(chunk, workload, chunk_id)\n",
    "                chunk.to_feather(f'{UNNORMALIZED_CHUNKED_DATA_DIR}/{chunk_id}.feather')\n",
    "                chunk_id += 1\n",
    "        metadata_dict[workload] = metadata\n",
    "           \n",
    "\n",
    "    with open(f'{UNNORMALIZED_CHUNKED_DATA_DIR}/group_metadata.json', 'w') as metadata_file:\n",
    "        json.dump(metadata_dict, metadata_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE TRAIN / TEST SPLIT\n",
    "Done at this point so the train / test sets can be normalized separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_files(files):\n",
    "    num_test = int(len(files) * 0.2)\n",
    "    test_files = np.random.choice(files, size=num_test, replace=False)\n",
    "    train_files = np.setdiff1d(files, test_files)\n",
    "    return train_files, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idxs(files, metadata, workload):\n",
    "    print(f'getting idxs for {workload}')\n",
    "    idxs = [idx for file in files for idx in range(metadata[file]['start_idx'], metadata[file]['end_idx'] + 1)]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = {}\n",
    "train_test_files = {}\n",
    "\n",
    "metadata_dict = json.load(open(f'{UNNORMALIZED_CHUNKED_DATA_DIR}/group_metadata.json'))\n",
    "\n",
    "for workload, metadata in metadata_dict.items():\n",
    "    print(workload)\n",
    "    files = np.array(list(metadata.keys()))\n",
    "    train_files, test_files = get_train_test_files(files)\n",
    "\n",
    "    train_idxs = get_idxs(train_files, metadata, workload)\n",
    "    test_idxs = get_idxs(test_files, metadata, workload)\n",
    "    train_test[workload] = {'train' : train_idxs, 'test' : test_idxs}\n",
    "    train_test_files[workload] = {'train': train_files.tolist(), 'test' : test_files.tolist()}\n",
    "\n",
    "with open('icache_consumed_chunked_data/train_test_idxs.json', 'w') as metadata_file:\n",
    "    json.dump(train_test, metadata_file, indent=4)\n",
    "\n",
    "with open('icache_consumed_chunked_data/train_test_files.json', 'w') as metadata_file:\n",
    "    json.dump(train_test_files, metadata_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCALE DATA\n",
    "Critical step for MLP!! Otherwise the network won't learn anything!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALED_DATA_DIR = \"icache_consumed_data/\"\n",
    "columns_to_scale = ['ft_start_addr', \n",
    "                    'ft_length', \n",
    "                    'ft_start_addr',\n",
    "                    'cycles_since_btb_rec', \n",
    "                    'cycles_since_ibtb_rec', \n",
    "                    'cycles_since_misfetch_rec',\n",
    "                    'cycles_since_mispred_rec',\n",
    "                    'btb_miss_rate',\n",
    "                    'ibtb_miss_rate',\n",
    "                    'misfetch_rate',\n",
    "                    'mispred_rate'\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale data with pre-defined chunking / train_test split\n",
    "# Fitting only on the train set prevents data leakage\n",
    "\n",
    "f = open('icache_consumed_data/group_metadata.json')\n",
    "metadata_dict = json.load(f)\n",
    "f = open('icache_consumed_data/train_test_idxs.json')\n",
    "train_test_idxs = json.load(f)\n",
    "f = open('icache_consumed_data/train_test_files.json')\n",
    "train_test_files = json.load(f)\n",
    "all_idxs = []\n",
    "for workload, metadata in metadata_dict.items():\n",
    "    print(workload)\n",
    "    # ????\n",
    "    train_idxs = train_test_idxs[workload]['train']\n",
    "    test_idxs = train_test_idxs[workload]['test']\n",
    "    train_files = metadata_file[workload]['train']\n",
    "    test_files = metadata_file[workload]['test']\n",
    "    # normalize\n",
    "    scaler = MinMaxScaler()\n",
    "    # fit on train set\n",
    "    train_df = pd.concat([pd.read_feather(f'{UNNORMALIZED_CHUNKED_DATA_DIR}/{file_no}.feather') for file_no in train_files])\n",
    "    scaler.fit(train_df[columns_to_scale])\n",
    "    del(train_df)\n",
    "    # transform on train and test set (save some memory)\n",
    "    for file_no in train_files:\n",
    "        df = pd.read_feather(f'{SCALED_DATA_DIR}/{file_no}.feather')\n",
    "        df[columns_to_scale] = scaler.transform(df[columns_to_scale])\n",
    "        df.to_feather(f'{SCALED_DATA_DIR}/{file_no}.feather')\n",
    "    for file_no in test_files:\n",
    "        df = pd.read_feather(f'{SCALED_DATA_DIR}/{file_no}.feather')\n",
    "        df[columns_to_scale] = scaler.transform(df[columns_to_scale])\n",
    "        df.to_feather(f'{SCALED_DATA_DIR}/{file_no}.feather')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bohr3_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
